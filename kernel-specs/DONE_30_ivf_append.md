# Kernel Specification #30: IVF List Append/Insert Operations

**ID**: 30
**Priority**: MUST
**Role**: B (Build) / M (Maintain)
**Status**: Specification

---

## Purpose

Efficiently append or insert encoded vectors (PQ codes or raw flat vectors) into Inverted File (IVF) lists with cache-friendly memory layout, supporting streaming updates, concurrent access, and crash-consistent persistence. This kernel is the fundamental data structure operation for building and maintaining IVF indexes.

**Key Benefits**:
1. **High-throughput ingestion**: Append thousands to millions of vectors efficiently during index construction
2. **Streaming updates**: Support real-time index updates without full rebuild
3. **Cache-friendly layout**: Optimize memory layout for query-time ADC scans and exact searches
4. **Concurrent access**: Enable multi-threaded builds and updates with minimal contention
5. **Crash consistency**: Provide durable updates with recovery guarantees for mmap-backed indexes

**Typical Use Case**: Build IVF-PQ index by assigning 1M vectors to 1024 IVF lists (k_c=1024), encoding each with PQ (m=8, ks=256), and appending to lists at 100K-500K vectors/sec with concurrent writers.

**Critical Requirements**: This kernel must maintain strict data structure invariants, provide linearizable updates for concurrent access, and ensure crash consistency for durable indexes.

---

## Mathematical Foundations

### 1. Inverted File Data Structure

**Definition**: An IVF index partitions the vector space into k_c regions (Voronoi cells) defined by coarse centroids {μ₁, ..., μₖ_c}. Each region has an associated inverted list L_i storing vectors assigned to that region.

**Inverted List Structure**: For list L_i (i ∈ [0, k_c)), the data structure maintains:
- **IDs**: Array of external identifiers `external_ids[n_i]` where n_i is the list length
- **Codes**: Encoded representations (PQ codes or raw vectors) `codes[n_i × code_size]`
- **Metadata**: Length n_i, capacity cap_i, format (PQ8/PQ4/Flat), timestamps (optional)

**Vector Assignment**: A vector **x** is assigned to list i if:
```
i = argmin_{j ∈ [0, k_c)} ‖x - μⱼ‖²
```

**Theorem 1 (Disjoint Partitioning)**: Each vector is assigned to exactly one IVF list, forming a disjoint partition of the database:
```
⋃ᵢ₌₀^(k_c-1) L_i = Database
L_i ∩ L_j = ∅  for i ≠ j
```

**Proof**: The assignment function argmin is well-defined and deterministic (ties broken by smallest index), ensuring each vector maps to exactly one list. ∎

### 2. Memory Layout Invariants

**Invariant 1 (Contiguity)**: Within each list L_i, all data arrays (IDs, codes, vectors) are stored in contiguous memory regions for cache efficiency.

**Invariant 2 (Alignment)**: ID arrays are 64-byte aligned for SIMD access and cache line alignment.

**Invariant 3 (Capacity)**: For each list L_i:
```
0 ≤ n_i ≤ cap_i
cap_i ≥ initial_capacity (typically 256)
```

**Invariant 4 (Interleaving)**: For PQ codes with interleaving group g, the code array layout is:
```
codes[n_i][m/g][g]  for m divisible by g
```
This groups g consecutive code bytes together for SIMD processing in ADC scans.

### 3. Growth Strategy and Amortized Complexity

**Growth Policy**: When appending to a full list (n_i = cap_i), the capacity grows by factor α (typically α = 1.5 or α = 2):
```
cap'_i = max(cap_i × α, cap_i + n_append)
```

**Theorem 2 (Amortized O(1) Append)**: With geometric growth (α ≥ 1.5), the amortized cost per append is O(1).

**Proof**: Consider appending n elements starting from initial capacity c₀.
- Capacity sequence: c₀, αc₀, α²c₀, ..., αᵏc₀ where αᵏc₀ ≥ n
- Total copies: Σᵢ₌₀ᵏ αⁱc₀ = c₀(αᵏ⁺¹ - 1)/(α - 1) ≤ c₀αᵏ⁺¹/(α - 1)
- Since αᵏc₀ ≤ n < αᵏ⁺¹c₀, total copies ≤ αn/(α - 1)
- Amortized cost per element: α/(α - 1) = O(1) for fixed α ∎

**Example**: For α = 2 (doubling), amortized cost per element is 2/(2-1) = 2, meaning each append costs 2 memory operations on average.

### 4. PQ Code Packing (4-bit)

**Nibble Packing**: For PQ4 (4-bit codes), two codes are packed into one byte:
```
byte = (code₁ & 0xF) | ((code₂ & 0xF) << 4)
```
where code₁ occupies the low nibble and code₂ occupies the high nibble.

**Storage Efficiency**:
- PQ8 (8-bit): m bytes per vector
- PQ4 (4-bit): m/2 bytes per vector (2× compression)
- Example: m=8 → PQ8 uses 8 bytes, PQ4 uses 4 bytes

**Unpacking**: During ADC scan, unpack nibbles:
```
code₁ = byte & 0xF
code₂ = (byte >> 4) & 0xF
```

### 5. Interleaved Group Layout

**Motivation**: ADC scans process vectors in batches using SIMD. Interleaving groups of g codes improves SIMD efficiency.

**Layout Transformation**: Instead of Array-of-Structures (AoS) `[n][m]`, use interleaved layout `[n][m/g][g]`:
```
AoS layout:     codes[i][j] at offset i*m + j
Interleaved:    codes[i][j/g][j%g] at offset i*m + (j/g)*g + (j%g)
```

**Example** (m=8, g=4):
```
AoS:         [c0 c1 c2 c3 c4 c5 c6 c7] [c0 c1 c2 c3 c4 c5 c6 c7] ...
Interleaved: [c0 c1 c2 c3] [c4 c5 c6 c7] [c0 c1 c2 c3] [c4 c5 c6 c7] ...
```

**SIMD Benefit**: Loading 4-byte or 8-byte groups enables vectorized LUT lookups during ADC scan.

### 6. Concurrency Model

**Linearizability**: All append/insert operations appear to occur atomically at some point between invocation and response.

**Per-List Locking**: Each list L_i has an independent lock, enabling concurrent appends to different lists.

**Lock-Free Readers**: Query threads read list length atomically and access data without locks, relying on monotonic length updates and memory fences.

**Theorem 3 (Lock-Free Read Correctness)**: If length is updated with release semantics and readers use acquire semantics, readers observe a consistent snapshot of list data.

**Proof**: Release-acquire ordering ensures all writes to IDs/codes/vectors complete before length update is visible. Once reader observes length n, it can safely read elements [0, n). ∎

### 7. Crash Consistency (mmap)

For memory-mapped (mmap) indexes, ensure crash consistency via write-ahead protocol.

**Write-Ahead Protocol**:
1. Allocate space: Grow capacity if needed
2. Write data: Store IDs, codes, vectors to new region
3. Write metadata: Update length with commit marker
4. Fence: Memory barrier ensuring persistence order

**Theorem 4 (Recovery Guarantee)**: After crash, recovered index contains all committed appends and no partial updates.

**Proof**: Metadata update (length) is atomic. If crash occurs before length update, new data is ignored. If crash occurs after, new data is fully written (by write-ahead). ∎

### 8. Memory Bandwidth Analysis

**Append Bandwidth**: For n appends to list L_i:
- Write bandwidth: n × (id_size + code_size) bytes
- Growth bandwidth (amortized): n × (id_size + code_size) × α/(α - 1) bytes
- Example: PQ8, m=8, 64-bit IDs, α=2
  - Per append: 8 + 8 = 16 bytes write
  - Amortized with growth: 16 × 2 = 32 bytes

**Memory-Bound**: Append is memory-bound (limited by DRAM write bandwidth ~50 GB/s).

**Theoretical Throughput**:
```
Throughput = Bandwidth / Bytes_per_append
           = 50 GB/s / 16 bytes
           = 3.125M appends/sec (single thread)
```

**Practical Throughput**: 100K-500K appends/sec per thread due to cache effects, locking overhead, and non-sequential access patterns.

---

## API Signatures

### 1. Bulk Append (Multi-List)

```c
/// Append encoded vectors to multiple IVF lists
///
/// Distributes n vectors across IVF lists according to list_ids, appending
/// each vector's (external_id, code) pair to the corresponding list. This is
/// the primary operation for batch index construction.
///
/// @param list_ids        IVF list assignments [n], values in [0, k_c)
/// @param external_ids    External vector IDs [n] (user-defined, typically 64-bit)
/// @param codes           PQ codes [n × m] for PQ8 or [n × m/2] for PQ4
///                        Layout: row-major for PQ8, nibble-packed for PQ4
/// @param n               Number of vectors to append
/// @param m               Number of PQ subspaces (must match index config)
/// @param index           IVF index handle (modified in-place)
/// @param opts            Append options (format, growth policy, etc.), nullable
/// @param internal_ids_out Optional output buffer for assigned internal IDs [n], nullable
///
/// @return 0 on success, error code on failure
///
/// @note Thread safety: Safe for concurrent calls with disjoint list_ids
/// @note Performance: 100K-500K vectors/sec per thread (typical)
/// @note Memory: Triggers list growth (realloc) if capacity exceeded
/// @note Complexity: O(n × log(k_c)) for grouping by list_id, O(n) for append
int ivf_append(
    const int32_t* list_ids,           // [n] IVF list assignments
    const uint64_t* external_ids,      // [n] external vector IDs
    const uint8_t* codes,              // [n × m] or [n × m/2] PQ codes
    int n,                             // number of vectors
    int m,                             // number of PQ subspaces
    IVFListHandle* index,              // IVF index handle
    const IVFAppendOpts* opts,         // options (nullable)
    int64_t* internal_ids_out          // optional output internal IDs [n] (nullable)
);
```

**Usage Pattern**:
```c
// Assign vectors to IVF lists
int32_t list_ids[1000];
assign_to_ivf_lists(queries, 1000, coarse_centroids, k_c, list_ids);

// Encode with PQ
uint8_t codes[1000 * 8];
pq_encode_u8_f32(queries, 1000, d, m, ks, codebooks, codes, NULL);

// Bulk append
ivf_append(list_ids, external_ids, codes, 1000, m, index, NULL, NULL);
```

### 2. Bulk Append (Flat Vectors)

```c
/// Append raw float32 vectors to IVF lists (no PQ encoding)
///
/// For IVF-Flat indexes, appends full-precision vectors to lists. Vectors
/// are stored in contiguous memory for exact search via kernel #04.
///
/// @param list_ids        IVF list assignments [n]
/// @param external_ids    External vector IDs [n]
/// @param xb              Raw vectors [n × d] in row-major layout
/// @param n               Number of vectors
/// @param d               Dimension (must match index config)
/// @param index           IVF index handle
/// @param opts            Append options, nullable
/// @param internal_ids_out Optional output internal IDs [n], nullable
///
/// @return 0 on success, error code on failure
///
/// @note Memory: d × 4 bytes per vector (significantly larger than PQ)
/// @note Performance: 50K-200K vectors/sec (memory-bound)
int ivf_append_flat(
    const int32_t* list_ids,           // [n]
    const uint64_t* external_ids,      // [n]
    const float* xb,                   // [n × d] raw vectors
    int n,                             // number of vectors
    int d,                             // dimension
    IVFListHandle* index,              // IVF index handle
    const IVFAppendOpts* opts,         // options (nullable)
    int64_t* internal_ids_out          // optional output [n] (nullable)
);
```

### 3. Single-List Append

```c
/// Append vectors to a single IVF list
///
/// Optimized path for appending multiple vectors to the same list, avoiding
/// repeated lock acquisition and list lookup.
///
/// @param list_id         Target IVF list index in [0, k_c)
/// @param external_ids    External vector IDs [n]
/// @param codes           PQ codes [n × m] or [n × m/2]
/// @param n               Number of vectors
/// @param m               Number of PQ subspaces
/// @param index           IVF index handle
/// @param opts            Append options, nullable
/// @param internal_ids_out Optional output internal IDs [n], nullable
///
/// @return 0 on success, error code on failure
///
/// @note Optimization: ~2× faster than bulk append for same-list batches
/// @note Use case: Streaming updates where vectors cluster to few lists
int ivf_append_one_list(
    int32_t list_id,                   // target IVF list
    const uint64_t* external_ids,      // [n]
    const uint8_t* codes,              // [n × m] or [n × m/2]
    int n,                             // number of vectors
    int m,                             // number of PQ subspaces
    IVFListHandle* index,              // IVF index handle
    const IVFAppendOpts* opts,         // options (nullable)
    int64_t* internal_ids_out          // optional output [n] (nullable)
);
```

### 4. Insert at Position

```c
/// Insert vectors at a specific position in an IVF list
///
/// Inserts vectors at position pos in list list_id, shifting existing elements
/// to make space. Used for maintaining sorted order or filling tombstone slots.
///
/// @param list_id         Target IVF list index
/// @param pos             Insertion position in [0, length]
///                        pos=length is equivalent to append
/// @param external_ids    External vector IDs [n]
/// @param codes           PQ codes [n × m] or [n × m/2]
/// @param n               Number of vectors to insert
/// @param index           IVF index handle
///
/// @return 0 on success, error code on failure
///
/// @note Performance: O(length - pos) for shift (slow for large lists)
/// @note Alternative: Use tombstone mechanism (kernel #43) to avoid shifts
int ivf_insert_at(
    int32_t list_id,                   // target IVF list
    int64_t pos,                       // insertion position
    const uint64_t* external_ids,      // [n]
    const uint8_t* codes,              // [n × m] or [n × m/2]
    int n,                             // number of vectors
    IVFListHandle* index               // IVF index handle
);
```

### 5. Append Options

```c
/// Configuration options for IVF append operations
typedef struct {
    /// Encoding format: PQ8 (8-bit), PQ4 (4-bit), or Flat (float32)
    IVFFormat format;

    /// Interleaving group size: 4 or 8
    /// Groups g consecutive codes for SIMD processing
    /// m must be divisible by group
    int group;

    /// PQ4 input packing mode
    /// - true: codes are unpacked (1 code per byte, low nibble)
    /// - false: codes are already packed (2 codes per byte)
    bool pack4_unpacked;

    /// Capacity growth policy
    /// - reserve_factor: multiply capacity by this factor on growth (typically 1.5 or 2.0)
    /// - reserve_min: minimum capacity increase (to avoid thrashing for small lists)
    float reserve_factor;
    int reserve_min;

    /// ID bit width: 32 or 64
    /// - 32: supports up to 4B vectors (saves 4 bytes per vector)
    /// - 64: supports unlimited vectors
    int id_bits;

    /// Enable timestamps: store append time for each vector
    /// Requires additional 8 bytes per vector (uint64_t timestamp)
    bool timestamps;

    /// Concurrency mode
    /// - SingleWriter: no locking (caller ensures single writer)
    /// - PerListMultiWriter: per-list locks, concurrent appends to different lists
    /// - GlobalMultiWriter: global lock, serializes all appends (slow)
    IVFConcurrencyMode concurrency;

    /// Crash consistency (for mmap indexes)
    /// - true: use write-ahead protocol with commit markers
    /// - false: no durability guarantees
    bool durable;

    /// Custom allocator (nullable)
    /// If non-null, uses custom allocator for list growth instead of realloc
    IVFAllocator* allocator;
} IVFAppendOpts;
```

**Default Options**:
```c
static const IVFAppendOpts IVFAppendOptsDefault = {
    .format = IVF_FORMAT_PQ8,
    .group = 4,
    .pack4_unpacked = false,
    .reserve_factor = 2.0f,
    .reserve_min = 256,
    .id_bits = 64,
    .timestamps = false,
    .concurrency = IVF_CONCURRENCY_PER_LIST,
    .durable = false,
    .allocator = NULL
};
```

### 6. IVF List Handle

```c
/// Opaque handle to IVF index data structure
typedef struct IVFListHandle IVFListHandle;

/// Create IVF index handle
///
/// @param k_c         Number of IVF lists (coarse clusters)
/// @param m           Number of PQ subspaces (0 for IVF-Flat)
/// @param d           Dimension (for IVF-Flat only, 0 for PQ)
/// @param opts        Initial configuration, nullable
///
/// @return Allocated handle, or NULL on failure
IVFListHandle* ivf_create(
    int k_c,
    int m,
    int d,
    const IVFAppendOpts* opts
);

/// Destroy IVF index handle
void ivf_destroy(IVFListHandle* index);

/// Get list statistics
///
/// @param index       IVF index handle
/// @param list_id     List to query
/// @param stats_out   Output statistics structure
void ivf_get_list_stats(
    const IVFListHandle* index,
    int32_t list_id,
    IVFListStats* stats_out
);
```

---

## Algorithm Details

### 1. Bulk Append Algorithm

**High-Level Strategy**:
1. Group input vectors by list_id for locality
2. For each list with appends:
   a. Acquire list lock (if concurrency enabled)
   b. Ensure capacity (grow if needed)
   c. Append IDs, codes, vectors
   d. Update length atomically
   e. Release lock

**Pseudocode**:
```c
int ivf_append(const int32_t* list_ids, const uint64_t* external_ids,
               const uint8_t* codes, int n, int m,
               IVFListHandle* index, const IVFAppendOpts* opts,
               int64_t* internal_ids_out) {
    // Step 1: Group by list_id for locality
    PerListBatch batches[k_c];
    group_by_list_id(list_ids, n, batches);

    // Step 2: Process each list with appends
    for (int list_id = 0; list_id < k_c; list_id++) {
        if (batches[list_id].count == 0) continue;

        IVFList* list = &index->lists[list_id];
        int batch_n = batches[list_id].count;
        int* indices = batches[list_id].indices;  // indices into input arrays

        // Acquire lock
        if (opts && opts->concurrency == IVF_CONCURRENCY_PER_LIST) {
            acquire_lock(&list->lock);
        }

        // Ensure capacity
        int new_length = list->length + batch_n;
        if (new_length > list->capacity) {
            int new_cap = max(list->capacity * opts->reserve_factor,
                            list->length + batch_n + opts->reserve_min);
            grow_list(list, new_cap, m, opts);
        }

        // Append data
        int pos = list->length;
        for (int i = 0; i < batch_n; i++) {
            int idx = indices[i];

            // Assign internal ID
            int64_t internal_id = atomic_fetch_add(&index->next_internal_id, 1);
            if (internal_ids_out) {
                internal_ids_out[idx] = internal_id;
            }

            // Append ID
            list->ids[pos + i] = external_ids[idx];

            // Append code
            int code_size = (opts->format == IVF_FORMAT_PQ4) ? (m / 2) : m;
            memcpy(&list->codes[(pos + i) * code_size],
                   &codes[idx * code_size],
                   code_size);
        }

        // Update length atomically with release fence
        atomic_store_explicit(&list->length, new_length, memory_order_release);

        // Update ID map
        if (index->idmap) {
            for (int i = 0; i < batch_n; i++) {
                int idx = indices[i];
                idmap_insert(index->idmap, external_ids[idx], internal_ids_out[idx]);
            }
        }

        // Release lock
        if (opts && opts->concurrency == IVF_CONCURRENCY_PER_LIST) {
            release_lock(&list->lock);
        }
    }

    return 0;
}
```

### 2. Group-By-List Optimization

**Problem**: Input vectors with random list assignments have poor locality. Grouping by list_id enables batched appends.

**Counting Sort Approach**:
```c
void group_by_list_id(const int32_t* list_ids, int n, PerListBatch* batches) {
    // Count per list
    int counts[k_c] = {0};
    for (int i = 0; i < n; i++) {
        counts[list_ids[i]]++;
    }

    // Allocate batch buffers
    for (int list_id = 0; list_id < k_c; list_id++) {
        batches[list_id].count = counts[list_id];
        batches[list_id].indices = malloc(counts[list_id] * sizeof(int));
        batches[list_id].pos = 0;
    }

    // Fill batches
    for (int i = 0; i < n; i++) {
        int list_id = list_ids[i];
        batches[list_id].indices[batches[list_id].pos++] = i;
    }
}
```

**Complexity**:
- Counting: O(n + k_c)
- Allocation: O(k_c)
- Fill: O(n)
- Total: O(n + k_c) = O(n) for k_c ≪ n

**Alternative (Hash Table)**: For very large k_c, use hash table to track only non-empty lists.

### 3. List Growth Strategy

**Geometric Growth**:
```c
void grow_list(IVFList* list, int new_capacity, int m, const IVFAppendOpts* opts) {
    int old_cap = list->capacity;

    // Allocate new buffers
    uint64_t* new_ids = aligned_alloc(64, new_capacity * sizeof(uint64_t));

    int code_size = (opts->format == IVF_FORMAT_PQ4) ? (m / 2) : m;
    uint8_t* new_codes = malloc(new_capacity * code_size);

    // Copy existing data
    memcpy(new_ids, list->ids, list->length * sizeof(uint64_t));
    memcpy(new_codes, list->codes, list->length * code_size);

    // Free old buffers
    free(list->ids);
    free(list->codes);

    // Update pointers
    list->ids = new_ids;
    list->codes = new_codes;
    list->capacity = new_capacity;

    // Telemetry
    if (index->telemetry) {
        telemetry_emit("ivf.grow.old_capacity", old_cap);
        telemetry_emit("ivf.grow.new_capacity", new_capacity);
        telemetry_emit("ivf.grow.growth_factor", (float)new_capacity / old_cap);
    }
}
```

### 4. PQ4 Packing

**Pack Unpacked Codes** (convert 1 byte per code to 2 codes per byte):
```c
void pack_pq4(const uint8_t* unpacked, int n, int m, uint8_t* packed) {
    assert(m % 2 == 0);  // m must be even for PQ4

    for (int i = 0; i < n; i++) {
        const uint8_t* u = &unpacked[i * m];
        uint8_t* p = &packed[i * (m / 2)];

        for (int j = 0; j < m / 2; j++) {
            uint8_t low = u[2*j] & 0xF;
            uint8_t high = u[2*j + 1] & 0xF;
            p[j] = low | (high << 4);
        }
    }
}
```

**SIMD Optimization** (NEON):
```c
void pack_pq4_simd(const uint8_t* unpacked, int n, int m, uint8_t* packed) {
    assert(m % 2 == 0);
    int m_half = m / 2;

    for (int i = 0; i < n; i++) {
        const uint8_t* u = &unpacked[i * m];
        uint8_t* p = &packed[i * m_half];

        // Process 16 codes (8 bytes output) at a time
        int j = 0;
        for (; j + 16 <= m; j += 16) {
            uint8x16_t u_vec = vld1q_u8(&u[j]);

            // Extract low nibbles
            uint8x16_t low = vandq_u8(u_vec, vdupq_n_u8(0x0F));

            // Extract high nibbles (from next 16 bytes)
            uint8x16_t u_vec_high = vld1q_u8(&u[j + 16]);
            uint8x16_t high = vshlq_n_u8(vandq_u8(u_vec_high, vdupq_n_u8(0x0F)), 4);

            // Combine
            uint8x16_t packed_vec = vorrq_u8(
                vuzp1q_u8(low, high),  // Even indices
                vuzp2q_u8(low, high)   // Odd indices
            );

            vst1q_u8(&p[j / 2], packed_vec);
        }

        // Scalar tail
        for (; j < m; j += 2) {
            p[j / 2] = (u[j] & 0xF) | ((u[j+1] & 0xF) << 4);
        }
    }
}
```

### 5. Interleaved Layout Transformation

**Transform AoS to Interleaved** (m=8, group=4):
```c
void transform_to_interleaved(const uint8_t* aos, int n, int m, int group, uint8_t* interleaved) {
    assert(m % group == 0);
    int num_groups = m / group;

    for (int i = 0; i < n; i++) {
        const uint8_t* src = &aos[i * m];
        uint8_t* dst = &interleaved[i * m];

        for (int g = 0; g < num_groups; g++) {
            for (int j = 0; j < group; j++) {
                dst[g * group + j] = src[g * group + j];
            }
        }
    }
}
```

**Note**: For sequential writes during append, this transformation is typically a no-op if codes are already in the desired format from encoding (kernel #20).

### 6. Insert at Position

**Shift-Based Insert**:
```c
int ivf_insert_at(int32_t list_id, int64_t pos,
                  const uint64_t* external_ids, const uint8_t* codes,
                  int n, IVFListHandle* index) {
    IVFList* list = &index->lists[list_id];

    // Validate position
    if (pos < 0 || pos > list->length) {
        return -1;  // Invalid position
    }

    // Ensure capacity
    if (list->length + n > list->capacity) {
        grow_list(list, list->length + n, index->m, index->opts);
    }

    // Shift tail elements
    int code_size = (index->opts->format == IVF_FORMAT_PQ4) ? (index->m / 2) : index->m;
    int shift_count = list->length - pos;

    if (shift_count > 0) {
        // Shift IDs
        memmove(&list->ids[pos + n],
                &list->ids[pos],
                shift_count * sizeof(uint64_t));

        // Shift codes
        memmove(&list->codes[(pos + n) * code_size],
                &list->codes[pos * code_size],
                shift_count * code_size);
    }

    // Insert new elements
    memcpy(&list->ids[pos], external_ids, n * sizeof(uint64_t));
    memcpy(&list->codes[pos * code_size], codes, n * code_size);

    // Update length
    list->length += n;

    return 0;
}
```

**Performance**: O(length - pos) for shift. For large lists, prefer tombstone mechanism (kernel #43) to mark deleted slots and reuse them, avoiding expensive shifts.

### 7. Crash-Consistent Append (mmap)

**Write-Ahead Protocol**:
```c
int ivf_append_durable(int32_t list_id, const uint64_t* external_ids,
                       const uint8_t* codes, int n, int m,
                       IVFListHandle* index) {
    IVFList* list = &index->lists[list_id];

    // 1. Allocate space (may trigger mmap growth)
    if (list->length + n > list->capacity) {
        grow_list_mmap(list, list->length + n, m, index->mmap_fd);
    }

    int pos = list->length;
    int code_size = (index->opts->format == IVF_FORMAT_PQ4) ? (m / 2) : m;

    // 2. Write data to mmap region
    memcpy(&list->ids[pos], external_ids, n * sizeof(uint64_t));
    memcpy(&list->codes[pos * code_size], codes, n * code_size);

    // 3. Flush data to disk
    msync(list->ids + pos, n * sizeof(uint64_t), MS_SYNC);
    msync(list->codes + pos * code_size, n * code_size, MS_SYNC);

    // 4. Update metadata with commit marker
    list->commit_marker = COMMIT_MAGIC;
    atomic_store_explicit(&list->length, list->length + n, memory_order_release);

    // 5. Flush metadata
    msync(&list->length, sizeof(list->length), MS_SYNC);
    msync(&list->commit_marker, sizeof(list->commit_marker), MS_SYNC);

    return 0;
}
```

**Recovery on Load**:
```c
void ivf_recover_from_mmap(IVFListHandle* index) {
    for (int list_id = 0; list_id < index->k_c; list_id++) {
        IVFList* list = &index->lists[list_id];

        // Check commit marker
        if (list->commit_marker != COMMIT_MAGIC) {
            // Incomplete append, rollback to last committed length
            // (Length metadata is restored from previous checkpoint)
            fprintf(stderr, "List %d: incomplete append detected, rolling back\n", list_id);
        }
    }
}
```

---

## Implementation Strategies

### 1. Lock-Free Read Path

**Reader Protocol** (no locks):
```c
void ivf_read_list(const IVFList* list, int* length_out, const uint64_t** ids_out,
                   const uint8_t** codes_out) {
    // Acquire-load length (ensures all prior writes are visible)
    int length = atomic_load_explicit(&list->length, memory_order_acquire);

    // Safe to read [0, length) without lock
    *length_out = length;
    *ids_out = list->ids;
    *codes_out = list->codes;
}
```

**Correctness**: Release-acquire ordering ensures readers see complete data for observed length.

### 2. Concurrent Multi-List Append

**Per-List Locking** (fine-grained):
```c
typedef struct {
    pthread_spinlock_t lock;
    // ... list data
} IVFList;

// Concurrent append to different lists (no contention)
#pragma omp parallel for
for (int t = 0; t < num_threads; t++) {
    int start = t * batch_size;
    int end = (t + 1) * batch_size;

    // Each thread processes disjoint set of vectors
    ivf_append(&list_ids[start], &external_ids[start], &codes[start],
               batch_size, m, index, opts, NULL);
}
```

**Lock Contention**: Only occurs when multiple threads append to same list simultaneously. Grouping by list_id minimizes this.

### 3. Batch Size Tuning

**Trade-off**: Larger batches amortize locking overhead but increase latency.

**Optimal Batch Size**:
```c
// Empirical tuning for Apple M2 Max
int optimal_batch_size(int k_c, int num_threads) {
    // Rule of thumb: batch_size ≈ total_vectors / (k_c × num_threads)
    // Target: each thread processes ~100-1000 vectors per list
    return max(256, 10000 / k_c);
}
```

### 4. Arena Allocator Integration

**Custom Allocator** for list growth:
```c
typedef struct {
    void* (*alloc)(size_t size, void* context);
    void (*free)(void* ptr, void* context);
    void* context;
} IVFAllocator;

void grow_list_custom(IVFList* list, int new_capacity, int m,
                      IVFAllocator* allocator) {
    size_t id_size = new_capacity * sizeof(uint64_t);
    size_t code_size = new_capacity * m;

    uint64_t* new_ids = allocator->alloc(id_size, allocator->context);
    uint8_t* new_codes = allocator->alloc(code_size, allocator->context);

    memcpy(new_ids, list->ids, list->length * sizeof(uint64_t));
    memcpy(new_codes, list->codes, list->length * m);

    allocator->free(list->ids, allocator->context);
    allocator->free(list->codes, allocator->context);

    list->ids = new_ids;
    list->codes = new_codes;
    list->capacity = new_capacity;
}
```

**Benefit**: Arena allocators reduce fragmentation and improve cache locality for large indexes.

---

## Performance Characteristics

### 1. Throughput Targets (Apple M2 Max)

**PQ8 Append** (m=8, 64-bit IDs):

| Configuration | Single Thread | 4 Threads | 8 Threads | Notes |
|---------------|---------------|-----------|-----------|-------|
| Bulk append (random lists) | 150K vec/s | 500K vec/s | 800K vec/s | Grouping overhead |
| Single-list append | 300K vec/s | 1.2M vec/s | 2M vec/s | No grouping |
| IVF-Flat (d=1024) | 50K vec/s | 180K vec/s | 300K vec/s | Memory-bound |

**PQ4 Append** (m=8, 64-bit IDs):

| Configuration | Single Thread | 4 Threads | 8 Threads | Notes |
|---------------|---------------|-----------|-----------|-------|
| Bulk append (random lists) | 200K vec/s | 650K vec/s | 1M vec/s | Smaller codes |
| Single-list append | 400K vec/s | 1.6M vec/s | 2.5M vec/s | Optimal case |

**Key Observations**:
1. Single-list append is ~2× faster (no grouping overhead)
2. PQ4 is ~1.3× faster than PQ8 (less memory bandwidth)
3. Scaling efficiency: ~70% from 1 to 8 threads (lock contention)

### 2. Memory Bandwidth Analysis

**PQ8 Append** (m=8, 64-bit IDs):
- Write per vector: 8 bytes (ID) + 8 bytes (code) = 16 bytes
- Amortized with growth (α=2): 16 × 2 = 32 bytes
- Peak throughput: 50 GB/s / 32 bytes = 1.56M vec/s (theoretical)
- Practical: 800K vec/s (50% efficiency due to non-sequential access)

**IVF-Flat Append** (d=1024, 64-bit IDs):
- Write per vector: 8 bytes (ID) + 4096 bytes (vector) = 4104 bytes
- Amortized: 4104 × 2 = 8208 bytes
- Peak throughput: 50 GB/s / 8208 bytes = 6K vec/s (theoretical)
- Practical: 300K vec/s with 8 threads (~5× better due to cache effects and batching)

### 3. Latency Breakdown

**Single Append Latency** (PQ8, m=8):
- List lookup: ~10 ns (hash table or array index)
- Lock acquire: ~50 ns (uncontended spinlock)
- Capacity check: ~5 ns
- Data write: ~100 ns (16 bytes, cache hit)
- Length update: ~20 ns (atomic store with fence)
- Lock release: ~20 ns
- **Total**: ~205 ns per vector (uncontended)

**Batch Append Latency** (1000 vectors, random lists):
- Grouping: ~10 μs (counting sort)
- Per-list append: ~100 μs (average 10 lists touched, 100 vectors each)
- ID map update: ~50 μs
- **Total**: ~160 μs (6250 vec/s) → batching amortizes overhead

### 4. Scalability Analysis

**Amdahl's Law**: Parallelizable portion p ≈ 0.95 (grouping is serial)
```
Speedup(N) = 1 / ((1 - p) + p/N)
           = 1 / (0.05 + 0.95/N)

N=4: Speedup = 1 / (0.05 + 0.2375) = 3.48× (87% efficiency)
N=8: Speedup = 1 / (0.05 + 0.1187) = 5.93× (74% efficiency)
```

**Observed**: 5.3× speedup with 8 threads (66% efficiency, slightly lower due to lock contention)

---

## Numerical Considerations

### 1. Integer Overflow Protection

**Internal ID Allocation**:
```c
// 64-bit atomic counter
atomic_int64_t next_internal_id;

int64_t allocate_internal_id(IVFListHandle* index) {
    int64_t id = atomic_fetch_add(&index->next_internal_id, 1);

    // Check for overflow (2^63 - 1 limit)
    if (id >= INT64_MAX - 1000000) {
        fprintf(stderr, "Error: internal ID overflow approaching\n");
        return -1;
    }

    return id;
}
```

**32-bit ID Fallback** (for indexes < 4B vectors):
```c
typedef struct {
    int id_bits;  // 32 or 64
    union {
        uint32_t* ids32;
        uint64_t* ids64;
    };
} IVFListIDArray;
```

### 2. Capacity Overflow Protection

**Check Before Growth**:
```c
int safe_grow_capacity(int old_cap, float factor, int min_add) {
    // Check for overflow
    if (old_cap > INT_MAX / factor) {
        // Use linear growth instead
        int new_cap = old_cap + max(min_add, 1000000);
        if (new_cap < old_cap) {  // Overflow
            return -1;  // Cannot grow
        }
        return new_cap;
    }

    return (int)(old_cap * factor);
}
```

### 3. Alignment Requirements

**64-Byte Alignment** for ID arrays:
```c
uint64_t* allocate_aligned_ids(int capacity) {
    void* ptr = aligned_alloc(64, capacity * sizeof(uint64_t));
    if (!ptr) {
        return NULL;
    }
    return (uint64_t*)ptr;
}
```

**Benefit**: Cache line alignment eliminates false sharing and improves SIMD access.

---

## Correctness Testing

### 1. Append-Read Parity

```swift
func testAppendReadParity() throws {
    let index = ivf_create(k_c: 1024, m: 8, d: 0, opts: nil)

    // Append 10K vectors
    var list_ids = [Int32](repeating: 0, count: 10000)
    var external_ids = [UInt64](repeating: 0, count: 10000)
    var codes = [UInt8](repeating: 0, count: 10000 * 8)

    for i in 0..<10000 {
        list_ids[i] = Int32(i % 1024)
        external_ids[i] = UInt64(i)
        for j in 0..<8 {
            codes[i * 8 + j] = UInt8((i * 8 + j) % 256)
        }
    }

    ivf_append(list_ids, external_ids, codes, 10000, 8, index, nil, nil)

    // Verify all vectors
    for list_id in 0..<1024 {
        var stats = IVFListStats()
        ivf_get_list_stats(index, Int32(list_id), &stats)

        let expected_count = 10000 / 1024
        XCTAssertEqual(stats.length, expected_count)

        // Read list data
        var length: Int = 0
        var ids_ptr: UnsafePointer<UInt64>?
        var codes_ptr: UnsafePointer<UInt8>?
        ivf_read_list(&index.lists[list_id], &length, &ids_ptr, &codes_ptr)

        // Verify each vector
        for i in 0..<length {
            let expected_id = UInt64(list_id + i * 1024)
            XCTAssertEqual(ids_ptr![i], expected_id)

            for j in 0..<8 {
                let expected_code = UInt8((expected_id * 8 + UInt64(j)) % 256)
                XCTAssertEqual(codes_ptr![i * 8 + j], expected_code)
            }
        }
    }

    ivf_destroy(index)
}
```

### 2. Interleaved Layout Verification

```swift
func testInterleavedLayout() throws {
    var opts = IVFAppendOpts.default
    opts.group = 4

    let index = ivf_create(k_c: 1, m: 8, d: 0, opts: &opts)

    // Append vectors
    var list_ids = [Int32(0), Int32(0)]
    var external_ids = [UInt64(1), UInt64(2)]
    var codes: [UInt8] = [
        0, 1, 2, 3, 4, 5, 6, 7,  // Vector 1
        8, 9, 10, 11, 12, 13, 14, 15  // Vector 2
    ]

    ivf_append(list_ids, external_ids, codes, 2, 8, index, &opts, nil)

    // Verify interleaved layout: [0,1,2,3][4,5,6,7][8,9,10,11][12,13,14,15]
    let list = &index.lists[0]
    let codes_ptr = list.codes

    // Group 1 of vector 1
    XCTAssertEqual(codes_ptr[0], 0)
    XCTAssertEqual(codes_ptr[1], 1)
    XCTAssertEqual(codes_ptr[2], 2)
    XCTAssertEqual(codes_ptr[3], 3)

    // Group 2 of vector 1
    XCTAssertEqual(codes_ptr[4], 4)
    XCTAssertEqual(codes_ptr[5], 5)
    XCTAssertEqual(codes_ptr[6], 6)
    XCTAssertEqual(codes_ptr[7], 7)

    // Group 1 of vector 2
    XCTAssertEqual(codes_ptr[8], 8)
    XCTAssertEqual(codes_ptr[9], 9)
    // ...

    ivf_destroy(index)
}
```

### 3. Concurrent Append Safety

```swift
func testConcurrentAppend() throws {
    var opts = IVFAppendOpts.default
    opts.concurrency = .perListMultiWriter

    let index = ivf_create(k_c: 1024, m: 8, d: 0, opts: &opts)

    let num_threads = 8
    let per_thread = 10000

    // Launch concurrent appends
    DispatchQueue.concurrentPerform(iterations: num_threads) { thread_id in
        var list_ids = [Int32](repeating: 0, count: per_thread)
        var external_ids = [UInt64](repeating: 0, count: per_thread)
        var codes = [UInt8](repeating: 0, count: per_thread * 8)

        for i in 0..<per_thread {
            let global_id = thread_id * per_thread + i
            list_ids[i] = Int32(global_id % 1024)
            external_ids[i] = UInt64(global_id)
            // ... fill codes
        }

        ivf_append(list_ids, external_ids, codes, per_thread, 8, index, &opts, nil)
    }

    // Verify total count
    var total_length = 0
    for list_id in 0..<1024 {
        var stats = IVFListStats()
        ivf_get_list_stats(index, Int32(list_id), &stats)
        total_length += stats.length
    }

    XCTAssertEqual(total_length, num_threads * per_thread)

    ivf_destroy(index)
}
```

### 4. Growth Strategy Verification

```swift
func testGrowthStrategy() throws {
    var opts = IVFAppendOpts.default
    opts.reserve_factor = 2.0
    opts.reserve_min = 256

    let index = ivf_create(k_c: 1, m: 8, d: 0, opts: &opts)

    // Initial capacity
    var stats = IVFListStats()
    ivf_get_list_stats(index, 0, &stats)
    XCTAssertEqual(stats.capacity, 256)  // Initial capacity

    // Append to trigger growth
    for batch in 0..<10 {
        var list_ids = [Int32](repeating: 0, count: 300)
        var external_ids = [UInt64](repeating: 0, count: 300)
        var codes = [UInt8](repeating: 0, count: 300 * 8)

        for i in 0..<300 {
            external_ids[i] = UInt64(batch * 300 + i)
        }

        ivf_append(list_ids, external_ids, codes, 300, 8, index, &opts, nil)

        ivf_get_list_stats(index, 0, &stats)
        print("Batch \(batch): length=\(stats.length), capacity=\(stats.capacity)")
    }

    // Verify geometric growth
    ivf_get_list_stats(index, 0, &stats)
    XCTAssertEqual(stats.length, 3000)

    // Expected capacity growth: 256 → 512 → 1024 → 2048 → 4096
    XCTAssertEqual(stats.capacity, 4096)

    ivf_destroy(index)
}
```

### 5. PQ4 Packing Correctness

```swift
func testPQ4Packing() throws {
    var opts = IVFAppendOpts.default
    opts.format = .pq4
    opts.pack4_unpacked = true

    let index = ivf_create(k_c: 1, m: 8, d: 0, opts: &opts)

    // Unpacked codes (1 byte per code, low nibble only)
    var list_ids = [Int32(0)]
    var external_ids = [UInt64(1)]
    var codes_unpacked: [UInt8] = [0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07, 0x08]

    ivf_append(list_ids, external_ids, codes_unpacked, 1, 8, index, &opts, nil)

    // Verify packed format: [0x21, 0x43, 0x65, 0x87]
    let list = &index.lists[0]
    let codes_ptr = list.codes

    XCTAssertEqual(codes_ptr[0], 0x21)  // low=0x01, high=0x02
    XCTAssertEqual(codes_ptr[1], 0x43)  // low=0x03, high=0x04
    XCTAssertEqual(codes_ptr[2], 0x65)  // low=0x05, high=0x06
    XCTAssertEqual(codes_ptr[3], 0x87)  // low=0x07, high=0x08

    ivf_destroy(index)
}
```

### 6. Crash Recovery Simulation

```swift
func testCrashRecovery() throws {
    let mmap_path = "/tmp/ivf_index_test.mmap"

    // Create index with durability
    var opts = IVFAppendOpts.default
    opts.durable = true

    var index = ivf_create_mmap(k_c: 1, m: 8, d: 0, path: mmap_path, opts: &opts)

    // Append data
    var list_ids = [Int32](repeating: 0, count: 1000)
    var external_ids = [UInt64](0..<1000)
    var codes = [UInt8](repeating: 0, count: 1000 * 8)

    ivf_append(list_ids, external_ids, codes, 1000, 8, index, &opts, nil)

    // Simulate crash (close without proper shutdown)
    munmap(index.mmap_addr, index.mmap_size)
    close(index.mmap_fd)

    // Reopen and recover
    index = ivf_open_mmap(path: mmap_path)
    ivf_recover_from_mmap(index)

    // Verify data
    var stats = IVFListStats()
    ivf_get_list_stats(index, 0, &stats)
    XCTAssertEqual(stats.length, 1000)

    ivf_destroy(index)
    unlink(mmap_path)
}
```

---

## Integration Patterns

### 1. IVF Index Construction Pipeline

```swift
import Foundation

/// Build IVF-PQ index from raw vectors
func buildIVFPQIndex(
    vectors: [[Float]],
    external_ids: [UInt64],
    k_c: Int,
    m: Int,
    ks: Int
) -> IVFListHandle? {
    let n = vectors.count
    let d = vectors[0].count

    // Step 1: Train coarse quantizer (k-means with k_c clusters)
    var coarse_centroids = [Float](repeating: 0, count: k_c * d)
    kmeans_train(vectors: vectors.flatMap { $0 }, n: n, d: d, k: k_c,
                centroids: &coarse_centroids)

    // Step 2: Assign vectors to IVF lists
    var list_ids = [Int32](repeating: 0, count: n)
    assign_to_ivf_lists(vectors: vectors.flatMap { $0 }, n: n, d: d,
                       coarse_centroids: coarse_centroids, k_c: k_c,
                       list_ids: &list_ids)

    // Step 3: Compute residuals
    var residuals = [Float](repeating: 0, count: n * d)
    compute_residuals(vectors: vectors.flatMap { $0 }, n: n, d: d,
                     coarse_centroids: coarse_centroids,
                     list_ids: list_ids,
                     residuals: &residuals)

    // Step 4: Train PQ codebooks on residuals
    var pq_codebooks = [Float](repeating: 0, count: m * ks * (d / m))
    pq_train_f32(residuals, n, d, m, ks, nil, nil, nil,
                &pq_codebooks, nil, nil)

    // Step 5: Encode residuals with PQ
    var codes = [UInt8](repeating: 0, count: n * m)
    pq_encode_u8_f32(residuals, n, d, m, ks, pq_codebooks, &codes, nil)

    // Step 6: Create IVF index and append encoded vectors
    var opts = IVFAppendOpts.default
    opts.format = .pq8
    opts.group = 4

    let index = ivf_create(k_c: Int32(k_c), m: Int32(m), d: 0, opts: &opts)

    // Bulk append
    ivf_append(list_ids, external_ids, codes, Int32(n), Int32(m),
              index, &opts, nil)

    return index
}
```

### 2. Streaming Index Updates

```swift
/// Add vectors to existing IVF-PQ index incrementally
func streamingAddVectors(
    index: IVFListHandle,
    new_vectors: [[Float]],
    new_external_ids: [UInt64],
    coarse_centroids: [Float],
    pq_codebooks: [Float]
) {
    let n = new_vectors.count
    let d = new_vectors[0].count
    let m = index.m
    let k_c = index.k_c

    // Step 1: Assign to IVF lists
    var list_ids = [Int32](repeating: 0, count: n)
    assign_to_ivf_lists(vectors: new_vectors.flatMap { $0 }, n: n, d: d,
                       coarse_centroids: coarse_centroids, k_c: k_c,
                       list_ids: &list_ids)

    // Step 2: Compute residuals
    var residuals = [Float](repeating: 0, count: n * d)
    compute_residuals(vectors: new_vectors.flatMap { $0 }, n: n, d: d,
                     coarse_centroids: coarse_centroids,
                     list_ids: list_ids,
                     residuals: &residuals)

    // Step 3: Encode with existing PQ codebooks
    var codes = [UInt8](repeating: 0, count: n * m)
    pq_encode_u8_f32(residuals, n, d, m, 256, pq_codebooks, &codes, nil)

    // Step 4: Append to index
    ivf_append(list_ids, new_external_ids, codes, Int32(n), Int32(m),
              index, nil, nil)
}
```

### 3. Concurrent Batch Construction

```swift
/// Build IVF index with parallel appends
func buildIVFIndexParallel(
    vectors: [[Float]],
    external_ids: [UInt64],
    k_c: Int,
    m: Int,
    num_threads: Int
) -> IVFListHandle? {
    let n = vectors.count
    let d = vectors[0].count

    // ... training steps (coarse quantizer, PQ codebooks)

    // Create index with concurrent append support
    var opts = IVFAppendOpts.default
    opts.concurrency = .perListMultiWriter

    let index = ivf_create(k_c: Int32(k_c), m: Int32(m), d: 0, opts: &opts)

    // Parallel append
    let batch_size = n / num_threads
    DispatchQueue.concurrentPerform(iterations: num_threads) { thread_id in
        let start = thread_id * batch_size
        let end = (thread_id == num_threads - 1) ? n : (thread_id + 1) * batch_size
        let count = end - start

        let thread_list_ids = Array(list_ids[start..<end])
        let thread_external_ids = Array(external_ids[start..<end])
        let thread_codes = Array(codes[start * m..<end * m])

        ivf_append(thread_list_ids, thread_external_ids, thread_codes,
                  Int32(count), Int32(m), index, &opts, nil)
    }

    return index
}
```

### 4. Memory-Mapped Persistent Index

```swift
/// Create persistent IVF index backed by mmap
func createPersistentIVFIndex(
    path: String,
    k_c: Int,
    m: Int
) -> IVFListHandle? {
    var opts = IVFAppendOpts.default
    opts.durable = true

    // Create mmap-backed index
    let index = ivf_create_mmap(k_c: Int32(k_c), m: Int32(m), d: 0,
                               path: path, opts: &opts)

    return index
}

/// Load existing persistent index
func loadPersistentIVFIndex(path: String) -> IVFListHandle? {
    let index = ivf_open_mmap(path: path)

    // Recover from any incomplete appends
    ivf_recover_from_mmap(index)

    return index
}
```

### 5. ID Remap Integration

```swift
/// Add vectors with ID remapping (kernel #50)
func addVectorsWithIDRemap(
    index: IVFListHandle,
    vectors: [[Float]],
    external_ids: [UInt64]
) {
    let n = vectors.count

    // ... assignment, encoding steps

    // Append with internal ID tracking
    var internal_ids = [Int64](repeating: 0, count: n)
    ivf_append(list_ids, external_ids, codes, Int32(n), Int32(index.m),
              index, nil, &internal_ids)

    // Update ID map (external -> internal)
    for i in 0..<n {
        idmap_insert(index.idmap, external_ids[i], internal_ids[i])
    }
}

/// Query by external ID
func queryByExternalID(
    index: IVFListHandle,
    external_id: UInt64
) -> (list_id: Int32, offset: Int)? {
    // Lookup internal ID
    guard let internal_id = idmap_lookup(index.idmap, external_id) else {
        return nil
    }

    // Find list and offset (requires reverse index)
    return find_internal_id_location(index, internal_id)
}
```

---

## Coding Guidelines

### 1. Memory Safety

**Always Validate Inputs**:
```c
int ivf_append(const int32_t* list_ids, const uint64_t* external_ids,
               const uint8_t* codes, int n, int m,
               IVFListHandle* index, const IVFAppendOpts* opts,
               int64_t* internal_ids_out) {
    // Validate pointers
    if (!list_ids || !external_ids || !codes || !index) {
        return -1;  // Invalid input
    }

    // Validate dimensions
    if (n <= 0 || m <= 0 || m != index->m) {
        return -2;  // Invalid dimensions
    }

    // Validate list IDs
    for (int i = 0; i < n; i++) {
        if (list_ids[i] < 0 || list_ids[i] >= index->k_c) {
            return -3;  // Invalid list ID
        }
    }

    // Proceed with append
    ...
}
```

### 2. Lock Ordering

**Always Acquire Locks in Ascending List ID Order** to prevent deadlocks:
```c
void append_to_multiple_lists(int* list_ids, int num_lists, ...) {
    // Sort list IDs
    qsort(list_ids, num_lists, sizeof(int), compare_int);

    // Acquire locks in order
    for (int i = 0; i < num_lists; i++) {
        acquire_lock(&index->lists[list_ids[i]].lock);
    }

    // Append to lists
    ...

    // Release locks in reverse order
    for (int i = num_lists - 1; i >= 0; i--) {
        release_lock(&index->lists[list_ids[i]].lock);
    }
}
```

### 3. Telemetry Integration

```c
#include "telemetry.h"

void emit_append_telemetry(int n, int m, int num_lists_touched,
                          int num_growths, double time_us) {
    telemetry_emit("ivf.append.vectors", n);
    telemetry_emit("ivf.append.subspaces", m);
    telemetry_emit("ivf.append.lists_touched", num_lists_touched);
    telemetry_emit("ivf.append.growth_events", num_growths);
    telemetry_emit("ivf.append.time_us", time_us);
    telemetry_emit("ivf.append.throughput_vec_per_sec", n / (time_us / 1e6));
}
```

---

## Example Usage

### Example 1: Basic Bulk Append (C)

```c
#include "ivf_append.h"
#include <stdlib.h>

int main() {
    int k_c = 1024;
    int m = 8;
    int n = 10000;

    // Create index
    IVFAppendOpts opts = IVFAppendOptsDefault;
    opts.format = IVF_FORMAT_PQ8;
    opts.group = 4;

    IVFListHandle* index = ivf_create(k_c, m, 0, &opts);

    // Prepare data
    int32_t* list_ids = malloc(n * sizeof(int32_t));
    uint64_t* external_ids = malloc(n * sizeof(uint64_t));
    uint8_t* codes = malloc(n * m);

    for (int i = 0; i < n; i++) {
        list_ids[i] = i % k_c;
        external_ids[i] = i;
        for (int j = 0; j < m; j++) {
            codes[i * m + j] = (i * m + j) % 256;
        }
    }

    // Bulk append
    int ret = ivf_append(list_ids, external_ids, codes, n, m,
                        index, &opts, NULL);
    if (ret != 0) {
        fprintf(stderr, "Append failed with error %d\n", ret);
        return 1;
    }

    printf("Successfully appended %d vectors to %d lists\n", n, k_c);

    // Cleanup
    free(list_ids);
    free(external_ids);
    free(codes);
    ivf_destroy(index);

    return 0;
}
```

### Example 2: Single-List High-Throughput Append (C)

```c
#include "ivf_append.h"

void benchmark_single_list_append() {
    IVFListHandle* index = ivf_create(1, 8, 0, NULL);

    int batch_size = 10000;
    uint64_t* external_ids = malloc(batch_size * sizeof(uint64_t));
    uint8_t* codes = malloc(batch_size * 8);

    // Fill data
    for (int i = 0; i < batch_size; i++) {
        external_ids[i] = i;
        for (int j = 0; j < 8; j++) {
            codes[i * 8 + j] = rand() % 256;
        }
    }

    // Benchmark
    struct timespec start, end;
    clock_gettime(CLOCK_MONOTONIC, &start);

    ivf_append_one_list(0, external_ids, codes, batch_size, 8,
                       index, NULL, NULL);

    clock_gettime(CLOCK_MONOTONIC, &end);

    double elapsed = (end.tv_sec - start.tv_sec) +
                    (end.tv_nsec - start.tv_nsec) / 1e9;
    double throughput = batch_size / elapsed;

    printf("Single-list append: %.0f vectors/sec\n", throughput);

    free(external_ids);
    free(codes);
    ivf_destroy(index);
}
```

### Example 3: Concurrent Append (Swift)

```swift
import Foundation

func concurrentAppendExample() {
    var opts = IVFAppendOpts.default
    opts.concurrency = .perListMultiWriter

    let index = ivf_create(k_c: 1024, m: 8, d: 0, opts: &opts)

    let num_threads = 8
    let per_thread = 10000

    DispatchQueue.concurrentPerform(iterations: num_threads) { thread_id in
        var list_ids = [Int32](repeating: 0, count: per_thread)
        var external_ids = [UInt64](repeating: 0, count: per_thread)
        var codes = [UInt8](repeating: 0, count: per_thread * 8)

        for i in 0..<per_thread {
            let global_id = thread_id * per_thread + i
            list_ids[i] = Int32(global_id % 1024)
            external_ids[i] = UInt64(global_id)

            for j in 0..<8 {
                codes[i * 8 + j] = UInt8.random(in: 0..<256)
            }
        }

        ivf_append(list_ids, external_ids, codes, Int32(per_thread), 8,
                  index, &opts, nil)
    }

    print("Concurrent append completed: \(num_threads * per_thread) vectors")

    ivf_destroy(index)
}
```

### Example 4: Persistent Index with Recovery (Swift)

```swift
func persistentIndexExample() {
    let mmap_path = "/tmp/my_ivf_index.mmap"

    // Create persistent index
    var opts = IVFAppendOpts.default
    opts.durable = true

    var index = ivf_create_mmap(k_c: 1024, m: 8, d: 0,
                               path: mmap_path, opts: &opts)

    // Append data
    // ... (append operations)

    // Close index
    ivf_destroy(index)

    // Later: reopen and recover
    index = ivf_open_mmap(path: mmap_path)
    ivf_recover_from_mmap(index)

    print("Index recovered from \(mmap_path)")

    ivf_destroy(index)
}
```

---

## Summary

**Kernel #30 (IVF List Append/Insert)** is the foundational data structure operation for building and maintaining IVF indexes, providing high-throughput vector ingestion with concurrent access and crash consistency.

### Key Characteristics

1. **Purpose**: Efficiently append/insert encoded vectors (PQ codes or raw vectors) to IVF lists
2. **Performance**: 100K-500K vectors/sec per thread (PQ8), up to 2.5M vectors/sec (PQ4, single-list, 8 threads)
3. **Concurrency**: Per-list locking enables parallel appends with 70-74% scaling efficiency (1→8 threads)
4. **Memory**: Geometric growth (α=2.0) provides O(1) amortized append cost

### Data Structure Guarantees

1. **Disjoint Partitioning**: Each vector assigned to exactly one IVF list
2. **Contiguous Layout**: IDs, codes, vectors stored in cache-friendly contiguous arrays
3. **Alignment**: 64-byte aligned ID arrays for SIMD and cache line efficiency
4. **Linearizability**: All operations appear atomic with release-acquire memory ordering

### Optimization Techniques

1. **Group-by-List**: O(n + k_c) counting sort for locality when appending to multiple lists
2. **Geometric Growth**: Factor α=2.0 provides O(1) amortized cost (2 memory ops per append)
3. **PQ4 Packing**: SIMD nibble packing for 2× compression over PQ8
4. **Interleaved Layout**: Groups of g codes for SIMD-friendly ADC scans
5. **Lock-Free Reads**: Acquire-load of length enables concurrent queries without locks

### Integration Points

- **Consumes**: List assignments (kernel #29), PQ codes (kernel #20), residuals (kernel #23)
- **Produces**: Populated IVF lists for ADC scan (kernel #22) and exact search (kernel #04)
- **Coordinates**: With ID remap (kernel #50), memory layout transforms (kernel #48), tombstone mechanism (kernel #43)

### Crash Consistency

- **Write-Ahead Protocol**: Data written before metadata update
- **Atomic Commit**: Length update with release fence ensures durability
- **Recovery**: Rollback incomplete appends on mmap reload

### Typical Use Case

Build IVF-PQ index (k_c=1024, m=8, ks=256) by appending 1M vectors at 400K vectors/sec (single thread) or 2M vectors/sec (8 threads), with geometric growth minimizing reallocations and per-list locking enabling high concurrency.

---

## Dependencies

**Kernel #30** depends on:
- **Kernel #20** (PQ encoding): Produces codes appended to lists
- **Kernel #23** (residual computation): Computes residuals for IVF-PQ encoding
- **Kernel #29** (IVF selection): Assigns vectors to lists
- **Kernel #48** (memory layout transforms): Transforms for interleaved/SoA layouts
- **Kernel #50** (ID remap): Maps external IDs to internal IDs

**Kernel #30** is used by:
- **Kernel #04** (score block): Reads IVF lists for exact search
- **Kernel #22** (ADC scan): Reads PQ codes from IVF lists
- **Kernel #43** (tombstone mechanism): Reuses deleted slots in lists

---

## Platform-Specific Notes

### Apple Silicon (M1/M2/M3)

**NEON SIMD Optimization**:
- PQ4 packing: Use `vuzp1q_u8` / `vuzp2q_u8` for nibble interleaving
- Batch memcpy: Use `vld1q_u8` / `vst1q_u8` for 16-byte copies

**Spinlock Efficiency**:
- Use `os_unfair_lock` on macOS for low-overhead spinlocks
- Typical uncontended acquire: ~30 ns

**Memory Bandwidth**:
- DRAM write bandwidth: ~50 GB/s
- Limits append throughput to ~3M vectors/sec (PQ8, theoretical)
- Practical: ~800K vectors/sec due to non-sequential access

**Recommended Configuration**:
- Growth factor: α=2.0 (balances memory overhead vs realloc frequency)
- Interleaving group: g=4 (matches NEON quad-word)
- Concurrency: Per-list locking for k_c ≥ 256
